# LLMs-Hallucination

**->Every day we use AI tools like ChatGPT, Gemini, or other assistants. Most of the time, they give useful answers. But sometimes, when they don’t know the real answer, instead of saying ‘I don’t know’, they actually make something up.


🧑 Imagine you ask a friend:
“Hey, do you know who invented the flying car?”

🚶 If your friend doesn’t actually know, instead of saying ‘I’m not sure’, they make something up like:
“Oh yeah, it was Elon Musk in 2010.”

It sounds convincing, but it’s completely wrong because flying cars (as we imagine them) don’t even exist yet.

🤖 AI hallucination is just like that friend—instead of admitting it doesn’t know, the AI sometimes guesses and gives you a wrong or made-up answer. 
