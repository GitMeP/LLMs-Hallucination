# LLMs-Hallucination

**->Every day we use AI tools like ChatGPT, Gemini, or other assistants. Most of the time, they give useful answers. But sometimes, when they donâ€™t know the real answer, instead of saying â€˜I donâ€™t knowâ€™, they actually make something up.


ğŸ§‘ Imagine you ask a friend:
â€œHey, do you know who invented the flying car?â€

ğŸš¶ If your friend doesnâ€™t actually know, instead of saying â€˜Iâ€™m not sureâ€™, they make something up like:
â€œOh yeah, it was Elon Musk in 2010.â€

It sounds convincing, but itâ€™s completely wrong because flying cars (as we imagine them) donâ€™t even exist yet.

ğŸ¤– AI hallucination is just like that friendâ€”instead of admitting it doesnâ€™t know, the AI sometimes guesses and gives you a wrong or made-up answer. 
