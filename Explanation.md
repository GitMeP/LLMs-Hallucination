# LLMs-Hallucination

## 🤖 What is Hallucination?

Every day we use AI tools like **ChatGPT**, **Gemini**, or other assistants.  
Most of the time, they give useful answers. But sometimes, when they don’t know the real answer, instead of saying *“I don’t know”*, they actually **make something up**.

I know, that sounds crazy but yeah, it happens! 🤯 

---

### 🧑 Example in Real Life
Imagine you ask a friend:  

> “Hey, do you know who invented the flying car?”  

🚶 If your friend doesn’t actually know, instead of saying *“I’m not sure”*, they make something up like:  

> “Oh yeah, it was Elon Musk in 2010.”  

It **sounds convincing**, but it’s completely wrong because flying cars (as we imagine them) don’t even exist yet.  

---

### 🤖 How it relates to AI
AI hallucination is just like that friend
Instead of admitting it doesn’t know, the AI sometimes **guesses** and gives you a **wrong or made-up answer**.  

---
