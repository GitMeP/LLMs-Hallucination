# LLMs-Hallucination

## ğŸ¤– What is Hallucination?

Every day we use AI tools like **ChatGPT**, **Gemini**, or other assistants.  
Most of the time, they give useful answers. But sometimes, when they donâ€™t know the real answer, instead of saying *â€œI donâ€™t knowâ€*, they actually **make something up**.

I know, that sounds crazy but yeah, it happens! ğŸ¤¯ 

---

### ğŸ§‘ Example in Real Life
Imagine you ask a friend:  

> â€œHey, do you know who invented the flying car?â€  

ğŸš¶ If your friend doesnâ€™t actually know, instead of saying *â€œIâ€™m not sureâ€*, they make something up like:  

> â€œOh yeah, it was Elon Musk in 2010.â€  

It **sounds convincing**, but itâ€™s completely wrong because flying cars (as we imagine them) donâ€™t even exist yet.  

---

### ğŸ¤– How it relates to AI
AI hallucination is just like that friend
Instead of admitting it doesnâ€™t know, the AI sometimes **guesses** and gives you a **wrong or made-up answer**.  

---
