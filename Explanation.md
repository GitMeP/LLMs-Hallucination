# LLMs-Hallucination

## 🤖 What is Hallucination?

Every day we use AI tools like **ChatGPT**, **Gemini**, or other assistants.  
Most of the time, they give useful answers. But sometimes, when they don’t know the real answer, instead of saying *“I don’t know”*, they actually **make something up**.

I know, that sounds crazy but yeah, it happens! 🤯 

---

### 🧑 Example in Real Life
Imagine you ask a friend:  

> “Hey, do you know who invented the flying car?”  

🚶 If your friend doesn’t actually know, instead of saying *“I’m not sure”*, they make something up like:  

> “Oh yeah, it was Elon Musk in 2010.”  

It **sounds convincing**, but it’s completely wrong because flying cars (as we imagine them) don’t even exist yet.  

---

### 🤖 How it relates to AI
AI hallucination is just like that friend
Instead of admitting it doesn’t know, the AI sometimes **guesses** and gives you a **wrong or made-up answer**.  

AI hallucinations are similar to how humans sometimes see figures in the clouds or faces on the moon. In the case of AI, these misinterpretations occur due to various factors, including overfitting, training data bias/inaccuracy and high model complexity.

* One significant source of hallucination in machine learning algorithms is input bias. If an AI model is trained on a dataset comprising biased or unrepresentative data, it may hallucinate patterns or features that reflect these biases.

---
